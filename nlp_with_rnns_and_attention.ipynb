{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "818dfdd9-e831-4b2d-a3fa-0e2f9e1c09ed",
   "metadata": {},
   "source": [
    "# RNN과 어텐션을 사용한 자연어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de7dbfd-3f08-4880-9c2b-d322a0c48cf2",
   "metadata": {},
   "source": [
    "먼저 몇 개의 모듈을 임포트한다. 맷플롯립 그림을 저장하는 함수를 준비한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "119fcb3d-8dd5-4779-9a8c-2caed0738f0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 공통 모듈 임포트\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 그림을 저장할 위치\n",
    "PROJECT_ROOT_DIR = '.'\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, 'images')\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension='png', resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, f'{fig_id}.{fig_extension}')\n",
    "    print(f'그림 저장 {fig_id}')\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, dpi=resolution, format=fig_extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840e8fa0-8d32-4ffd-a697-8e805aa2ca32",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Char-RNN을 사용해 셰익스피어 문제 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd897f3-c480-4c36-a094-fa2b7d223213",
   "metadata": {},
   "source": [
    "예를 들어, 0~14까지 시퀀스를 2개씩 이동하면서 길이가 5인 윈도우로 나누어 본다(가령,`[0, 1, 2, 3, 4]`, `[2, 3, 4, 5, 6]`, 등). 그다음 이를 섞고 입력(처음 네 개의 스텝)과 타깃(마지막 네 개의 스텝)으로 나눈다(즉, `[2, 3, 4, 5, 6]`를 `[[2, 3, 4, 5], [3, 4, 5, 6]]`로 나눈다). 그다음 입력/타깃 쌍 세 개로 구성된 배치를 만든다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "363dbd9f-18de-4b18-9e19-3e3bc3504a53",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ Batch 0\n",
      "X_batch\n",
      "[[10 11 12 13]\n",
      " [ 8  9 10 11]\n",
      " [ 0  1  2  3]]\n",
      "=====\n",
      "Y_batch\n",
      "[[11 12 13 14]\n",
      " [ 9 10 11 12]\n",
      " [ 1  2  3  4]]\n",
      "____________________ Batch 1\n",
      "X_batch\n",
      "[[2 3 4 5]\n",
      " [4 5 6 7]\n",
      " [6 7 8 9]]\n",
      "=====\n",
      "Y_batch\n",
      "[[ 3  4  5  6]\n",
      " [ 5  6  7  8]\n",
      " [ 7  8  9 10]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_steps = 5\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(15))\n",
    "dataset = dataset.window(n_steps, 2, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(n_steps))\n",
    "dataset = dataset.shuffle(10).map(lambda window: (window[:-1], window[1:]))\n",
    "dataset = dataset.batch(3).prefetch(tf.data.AUTOTUNE)\n",
    "for index, (X_batch, Y_batch) in enumerate(dataset):\n",
    "    print(f'{\"_\" * 20} Batch {index}\\nX_batch\\n{X_batch.numpy()}\\n{\"=\" * 5}\\nY_batch\\n{Y_batch.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea6087a-39f5-4e7c-b415-d0db9798eac5",
   "metadata": {},
   "source": [
    "### 훈련 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "091c6705-e0c6-4003-a516-8108a01fed76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "shakespeare_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "filepath = keras.utils.get_file('shakespeare.txt', shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4866e867-017f-43d4-a06e-6a0cc5430043",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[:148])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f4c97fd-e9ec-40b0-ab76-e411f66ac90e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(sorted(set(shakespeare_text.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d7591a9-eeab-4975-a40a-ad41fb914627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74645fc5-22b8-4dbc-982c-cd8a226f1bfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['First'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cd5fbff-f1b2-4cfa-9131-2578ebedce6a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9212b24a-4978-474e-ae50-3fecb38c003c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_id = len(tokenizer.word_index)  # 고유한 문자 개수\n",
    "dataset_size = tokenizer.document_count  # 전체 문자 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5aa3f3b4-83d1-4216-a271-f1bbbe2ed67a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d407efb6-37c9-4815-a4b7-4ed9e12a1dbf",
   "metadata": {},
   "source": [
    "### 순차 데이터셋을 나누는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "319cfaeb-6e85-4df6-b81e-d3eeb3a605ee",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5ebf77-da20-4798-b3b5-7ddccc7ca003",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 순차 데이터를 윈도 여러 개로 자르기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6746eaac-5470-44d4-9e78-1a8404c0941b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1  # 타깃 = 한 글자 앞선 입력\n",
    "dataset = dataset.window(window_length, 1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77585957-6358-4b08-9f7e-395846f81e57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0d6199d-edad-4c23-8526-62b4ca04081b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d7e4415-9e1f-484e-902d-7111c03271d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48711eb7-5216-4ad7-921c-6b35c5ef0b0c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1be07385-d55f-4235-a659-80fa840a9ea4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45438362-ed83-4fcd-8eeb-58fb6e79d9ef",
   "metadata": {},
   "source": [
    "### Char-RNN 모델 만들고 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34525f55-9149-4a84-a31c-e53bf0ddf82a",
   "metadata": {},
   "source": [
    "**경고**: 다음 코드는 하드웨어에 따라 실행하는데 24시간이 걸릴 수 있다. GPU를 사용하면 1~2시간 정도 걸릴 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63545788-9fbf-4fa8-b6f3-07b547386878",
   "metadata": {},
   "source": [
    "**노트**: `GRU` 클래스는 다음 매개변수에서 기본값을 사용할 때에만 GPU를 사용한다: `activation`, `recurrent_activation`, `recurrent_dropout`, `unroll`, `use_bias` `reset_after`. 이 때문에 `recurrent_dropout=0.2`를 주석 처리했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9c105da-327f-4b26-a87e-3fa06a88add9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 162s 27ms/step - loss: 1.7840\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential(\n",
    "    [\n",
    "        keras.layers.GRU(128, dropout=0.2, return_sequences=True, input_shape=[None, max_id]),  # recurrent_dropout=0.2\n",
    "        keras.layers.GRU(128, dropout=0.2, return_sequences=True),  # recurrent_dropout=0.2\n",
    "        keras.layers.TimeDistributed(keras.layers.Dense(max_id, 'softmax'))\n",
    "    ]\n",
    ")\n",
    "model.compile('adam', 'sparse_categorical_crossentropy')\n",
    "# history = model.fit(dataset, epochs=10)\n",
    "history = model.fit(dataset.take(2222))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e065685-b6cd-4a40-945d-a43be9df65e7",
   "metadata": {},
   "source": [
    "### Char-RNN 모델 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92db38bc-5e31-487f-b7a0-a7965cc6732e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f850de9-9857-48dd-a58d-b38c210509bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = preprocess(['How are yo'])\n",
    "Y_pred = np.argmax(model(X_new), -1)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]  # 1st sentence, last char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3205dcb-76d5-475e-921f-d6f1fc6ce18c",
   "metadata": {},
   "source": [
    "### 가짜 셰익스피어 텍스트를 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "075cab5a-19d6-44b7-bd4a-30780e3edc62",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "        0, 0, 0, 1, 1, 2, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 2, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], 40).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7357390-ed83-44f6-88a4-c93089165cac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, 1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "761ca93d-3959-424a-9fe2-f8929ab4f799",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_char('How are yo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5274a4ee-be22-4949-8752-16ec8ab9e46c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9586bbfc-475f-45ca-8578-4796064fe97c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the country's send ender that i have beard the cous\n"
     ]
    }
   ],
   "source": [
    "print(complete_text('t', temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ab2af9f-db86-427c-9d58-c8b72476bc00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "til'd so,\n",
      "as i lord, fair.\n",
      "my lear me dreat toke, t\n"
     ]
    }
   ],
   "source": [
    "print(complete_text('t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f33f91e6-bd8b-4945-9df6-0dbab9587e9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thaovsemt-libtamudh\n",
      "of jups betoulm\n",
      "it'surlf-\n",
      "osiri\n"
     ]
    }
   ],
   "source": [
    "print(complete_text('t', temperature=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d674e-29c2-48aa-8164-e2bb03cb7104",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 상태가 있는 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2364acd7-6947-4793-acf1-e2aece317f0e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(window_length, n_steps, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset = dataset.batch(1)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, max_id), Y_batch))\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "391858c9-8050-45e5-8798-afb93b63de16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
    "datasets = []\n",
    "for encoded_part in encoded_parts:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
    "    dataset = dataset.window(window_length, n_steps, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "    datasets.append(dataset)\n",
    "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, max_id), Y_batch))\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740acef2-9e51-47e6-a038-9efa102bac50",
   "metadata": {},
   "source": [
    "**노트**: 여기에서도 GPU 가속을 위해 `recurrent_dropout=0.2`을 주석 처리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "517ab9f9-7902-4b73-8c51-e308ce987dcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential(\n",
    "    [\n",
    "        keras.layers.GRU(\n",
    "            128, dropout=0.2, return_sequences=True, stateful=True, batch_input_shape=[batch_size, None, max_id]\n",
    "        ),  # recurrent_dropout=0.2\n",
    "        keras.layers.GRU(128, dropout=0.2, return_sequences=True, stateful=True),  # recurrent_dropout=0.2\n",
    "        keras.layers.TimeDistributed(keras.layers.Dense(max_id, 'softmax'))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "240d56e8-3f0f-4c11-b073-e5b6522d5c87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6b13ecb-92a0-4a1b-9958-aab9dab9e96a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "313/313 [==============================] - 10s 27ms/step - loss: 2.6227\n",
      "Epoch 2/15\n",
      "313/313 [==============================] - 8s 26ms/step - loss: 2.2504\n",
      "Epoch 3/15\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 2.1199\n",
      "Epoch 4/15\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 2.0435\n",
      "Epoch 5/15\n",
      "313/313 [==============================] - 9s 27ms/step - loss: 1.9913\n",
      "Epoch 6/15\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 1.9527\n",
      "Epoch 7/15\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 1.9239\n",
      "Epoch 8/15\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 1.9011\n",
      "Epoch 9/15\n",
      "313/313 [==============================] - 9s 27ms/step - loss: 1.8812\n",
      "Epoch 10/15\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 1.8665\n",
      "Epoch 11/15\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 1.8538\n",
      "Epoch 12/15\n",
      "313/313 [==============================] - 9s 27ms/step - loss: 1.8435\n",
      "Epoch 13/15\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 1.8340\n",
      "Epoch 14/15\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 1.8223\n",
      "Epoch 15/15\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 1.8156\n"
     ]
    }
   ],
   "source": [
    "model.compile('adam', 'sparse_categorical_crossentropy')\n",
    "# history = model.fit(dataset, epochs=50, callbacks=[ResetStatesCallback()])\n",
    "history = model.fit(dataset, epochs=7, callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d614c188-8f9b-4076-999d-fea2540d41a0",
   "metadata": {},
   "source": [
    "모델에 다른 크기의 배치를 사용하려면 상태가 없는 복사본을 만들어야 한다. 드롭아웃은 훈련에만 사용되기 때문에 삭제한다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79a47e3a-35a4-458e-b188-444f188a7bd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stateless_model = keras.models.Sequential(\n",
    "    [\n",
    "        keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
    "        keras.layers.GRU(128, return_sequences=True),\n",
    "        keras.layers.TimeDistributed(keras.layers.Dense(max_id, 'softmax'))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceba5567-8ea2-4fb6-818f-8ab2407b1d36",
   "metadata": {},
   "source": [
    "가중치를 복사하려면 먼저 (가중치를 만들기 위해) 모델을 빌드한다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5dd5bf9-4aee-41a0-9a54-87bd3794cc44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stateless_model.build(tf.TensorShape([None, None, max_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c32afe4-a086-4184-bf82-5ec99c6ff4b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stateless_model.set_weights(model.get_weights())\n",
    "model = stateless_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8af05c2-655f-4d78-94c6-9884de53fcb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t,\n",
      "'tis have like anfless, i can strenghher vanch'd\n"
     ]
    }
   ],
   "source": [
    "print(complete_text('t'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5808f2c-e36b-41c4-ac89-c702ad5fff03",
   "metadata": {},
   "source": [
    "## 감성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16c830c6-872e-4830-8dc2-7be3a8b1a27a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb45b349-13af-42f2-85ff-46d604283070",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dec14aa2-0a32-41d9-980f-ca3b8516271f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> this film was just brilliant casting location scenery story'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "for id_, token in enumerate(('<pad>', '<sos>', '<unk>')):\n",
    "    id_to_word[id_] = token\n",
    "' '.join([id_to_word[id_] for id_ in X_train[0][:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad6b9a52-a82a-469d-901b-8f13af6d6335",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load('imdb_reviews', as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "511b6565-5b35-43f5-972c-8949c0d8a7dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test', 'unsupervised'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d313ab2d-949d-4d74-9b2a-3ce512334e67",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_size = info.splits['train'].num_examples\n",
    "test_size = info.splits['test'].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7aa9a4b6-0662-4f70-b164-8eaf244049aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "55c4d675-1017-4f11-8304-2a745ef723b9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting ...\n",
      "Label: 0 = Negative\n",
      "\n",
      "Review: I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However ...\n",
      "Label: 0 = Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in datasets['train'].batch(2).take(1):\n",
    "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
    "        print(f'Review: {review.decode(\"utf-8\")[:200]}...\\nLabel: {label} = {\"Positive\" if label else \"Negative\"}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28d32825-d84f-48a3-aefa-081e941ac352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b'<br\\s*/?>', b' ')\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b' ')\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    return X_batch.to_tensor(b'<pad>'), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe60d6c9-8421-479b-b6b1-feb21f1ae78b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 53), dtype=string, numpy=\n",
       " array([[b'This', b'was', b'an', b'absolutely', b'terrible', b'movie',\n",
       "         b\"Don't\", b'be', b'lured', b'in', b'by', b'Christopher',\n",
       "         b'Walken', b'or', b'Michael', b'Ironside', b'Both', b'are',\n",
       "         b'great', b'actors', b'but', b'this', b'must', b'simply', b'be',\n",
       "         b'their', b'worst', b'role', b'in', b'history', b'Even',\n",
       "         b'their', b'great', b'acting', b'could', b'not', b'redeem',\n",
       "         b'this', b\"movie's\", b'ridiculous', b'storyline', b'This',\n",
       "         b'movie', b'is', b'an', b'early', b'nineties', b'US',\n",
       "         b'propaganda', b'pi', b'<pad>', b'<pad>', b'<pad>'],\n",
       "        [b'I', b'have', b'been', b'known', b'to', b'fall', b'asleep',\n",
       "         b'during', b'films', b'but', b'this', b'is', b'usually', b'due',\n",
       "         b'to', b'a', b'combination', b'of', b'things', b'including',\n",
       "         b'really', b'tired', b'being', b'warm', b'and', b'comfortable',\n",
       "         b'on', b'the', b'sette', b'and', b'having', b'just', b'eaten',\n",
       "         b'a', b'lot', b'However', b'on', b'this', b'occasion', b'I',\n",
       "         b'fell', b'asleep', b'because', b'the', b'film', b'was',\n",
       "         b'rubbish', b'The', b'plot', b'development', b'was', b'constant',\n",
       "         b'Cons']], dtype=object)>,\n",
       " <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 0], dtype=int64)>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97d5fbb6-cb73-4c6f-8505-a0bdbd2a5c96",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = Counter()\n",
    "for X_batch, _ in datasets['train'].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "649641cb-b3ff-4225-a2b8-47c98e02cce6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.most_common()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e005907d-8efd-4b0a-9556-8dbb54492da6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53893"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b1316dab-c9ed-43ee-8472-014e4bfdd9e0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "truncated_vocabulary = [word for word, _ in vocabulary.most_common()[:vocab_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5dd27652-8d5f-415f-a762-5f4d27bc8928",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "12\n",
      "11\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}\n",
    "for word in b'This movie was faaaaaantastic'.split():\n",
    "    print(word_to_id.get(word) or vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d2224486-e46a-488a-8455-45a7c9b766b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e7f468bd-6f6d-4fb8-83e1-5b36a6a88018",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   22,    12,    11, 10053]], dtype=int64)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.lookup(tf.constant([b'This movie was faaaaaantastic'.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "233d2f54-cd7e-412b-9465-defed7f3a342",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "\n",
    "train_set = datasets['train'].batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e4702820-d989-4b29-90ee-9a55dc9a52f3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  22   11   28 ...    0    0    0]\n",
      " [   6   21   70 ...    0    0    0]\n",
      " [4099 6881    1 ...    0    0    0]\n",
      " ...\n",
      " [  22   12  118 ...  331 1047    0]\n",
      " [1757 4101  451 ...    0    0    0]\n",
      " [3365 4392    6 ...    0    0    0]], shape=(32, 60), dtype=int64)\n",
      "tf.Tensor([0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0], shape=(32,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in train_set.take(1):\n",
    "    print(X_batch)\n",
    "    print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8156c98c-4080-49b2-82ac-21a166185011",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "782/782 [==============================] - 27s 26ms/step - loss: 0.5356 - accuracy: 0.7263\n",
      "Epoch 2/4\n",
      "782/782 [==============================] - 20s 25ms/step - loss: 0.3486 - accuracy: 0.8550\n",
      "Epoch 3/4\n",
      "782/782 [==============================] - 20s 25ms/step - loss: 0.1930 - accuracy: 0.9310\n",
      "Epoch 4/4\n",
      "782/782 [==============================] - 20s 25ms/step - loss: 0.1369 - accuracy: 0.9513\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "model = keras.models.Sequential(\n",
    "    [\n",
    "        keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size, mask_zero=True, input_shape=[None]),\n",
    "        keras.layers.GRU(128, return_sequences=True),\n",
    "        keras.layers.GRU(128),\n",
    "        keras.layers.Dense(1, 'sigmoid')\n",
    "    ]\n",
    ")\n",
    "model.compile('adam', 'binary_crossentropy', ['accuracy'])\n",
    "# history = model.fit(train_set, epochs=5)\n",
    "history = model.fit(train_set, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dcc8a6-2fcd-4b41-8fd9-1288c56f573d",
   "metadata": {},
   "source": [
    "또는 직접 마스킹을 한다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "72340150-8ed0-4c26-8f61-ebf4c875f0cb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "782/782 [==============================] - 27s 25ms/step - loss: 0.5337 - accuracy: 0.7246\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.3482 - accuracy: 0.8545\n"
     ]
    }
   ],
   "source": [
    "K = keras.backend\n",
    "embed_size = 128\n",
    "inputs = keras.layers.Input([None])\n",
    "mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
    "z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
    "z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\n",
    "z = keras.layers.GRU(128)(z, mask=mask)\n",
    "outputs = keras.layers.Dense(1, 'sigmoid')(z)\n",
    "model = keras.models.Model([inputs], [outputs])\n",
    "model.compile('adam', 'binary_crossentropy', ['accuracy'])\n",
    "# history = model.fit(train_set, epochs=5)\n",
    "history = model.fit(train_set, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc01980-ffdd-4785-8153-f95e528bad0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
