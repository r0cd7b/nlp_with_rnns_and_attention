{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "818dfdd9-e831-4b2d-a3fa-0e2f9e1c09ed",
   "metadata": {},
   "source": [
    "# RNN과 어텐션을 사용한 자연어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4010efc5-ca31-4a68-ab00-a2e02387f3cb",
   "metadata": {},
   "source": [
    "**설정**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de7dbfd-3f08-4880-9c2b-d322a0c48cf2",
   "metadata": {},
   "source": [
    "먼저 몇 개의 모듈을 임포트한다. 맷플롯립 그림을 저장하는 함수를 준비한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "119fcb3d-8dd5-4779-9a8c-2caed0738f0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 공통 모듈 임포트\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 그림을 저장할 위치\n",
    "PROJECT_ROOT_DIR = '.'\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, 'images')\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension='png', resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + '.' + fig_extension)\n",
    "    print(f'그림 저장 {fig_id}')\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, dpi=resolution, format=fig_extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840e8fa0-8d32-4ffd-a697-8e805aa2ca32",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Char-RNN을 사용해 셰익스피어 문제 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa11fe7c-f3be-4e19-986d-d2168ae3677d",
   "metadata": {},
   "source": [
    "**시퀀스를 셔플 윈도우 배치로 나누기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd897f3-c480-4c36-a094-fa2b7d223213",
   "metadata": {},
   "source": [
    "예를 들어, 0~14까지 시퀀스를 2개씩 이동하면서 길이가 5인 윈도우로 나누어 본다(가령,`[0, 1, 2, 3, 4]`, `[2, 3, 4, 5, 6]`, 등). 그다음 이를 섞고 입력(처음 네 개의 스텝)과 타깃(마지막 네 개의 스텝)으로 나눈다(즉, `[2, 3, 4, 5, 6]`를 `[[2, 3, 4, 5], [3, 4, 5, 6]]`로 나눈다). 그다음 입력/타깃 쌍 세 개로 구성된 배치를 만든다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "363dbd9f-18de-4b18-9e19-3e3bc3504a53",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ Batch 0\n",
      "X_batch\n",
      "[[ 2  3  4  5]\n",
      " [ 8  9 10 11]\n",
      " [ 6  7  8  9]]\n",
      "=====\n",
      "Y_batch\n",
      "[[ 3  4  5  6]\n",
      " [ 9 10 11 12]\n",
      " [ 7  8  9 10]]\n",
      "____________________ Batch 1\n",
      "X_batch\n",
      "[[ 0  1  2  3]\n",
      " [10 11 12 13]\n",
      " [ 4  5  6  7]]\n",
      "=====\n",
      "Y_batch\n",
      "[[ 1  2  3  4]\n",
      " [11 12 13 14]\n",
      " [ 5  6  7  8]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_steps = 5\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(15))\n",
    "dataset = dataset.window(n_steps, 2, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(n_steps))\n",
    "dataset = dataset.shuffle(10).map(lambda window: (window[:-1], window[1:]))\n",
    "dataset = dataset.batch(3).prefetch(tf.data.AUTOTUNE)\n",
    "for index, (X_batch, Y_batch) in enumerate(dataset):\n",
    "    print(f'{\"_\" * 20} Batch {index}\\nX_batch\\n{X_batch.numpy()}\\n{\"=\" * 5}\\nY_batch\\n{Y_batch.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea6087a-39f5-4e7c-b415-d0db9798eac5",
   "metadata": {},
   "source": [
    "### 훈련 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef354df3-3bf9-4f14-bf94-c6eff032fbae",
   "metadata": {},
   "source": [
    "**데이터 로드하고 데이터셋 준비하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "091c6705-e0c6-4003-a516-8108a01fed76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "shakespeare_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "filepath = keras.utils.get_file('shakespeare.txt', shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4866e867-017f-43d4-a06e-6a0cc5430043",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[:148])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8f4c97fd-e9ec-40b0-ab76-e411f66ac90e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(sorted(set(shakespeare_text.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8d7591a9-eeab-4975-a40a-ad41fb914627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "74645fc5-22b8-4dbc-982c-cd8a226f1bfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['First'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5cd5fbff-f1b2-4cfa-9131-2578ebedce6a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9212b24a-4978-474e-ae50-3fecb38c003c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_id = len(tokenizer.word_index)  # 고유한 문자 개수\n",
    "dataset_size = tokenizer.document_count  # 전체 문자 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5aa3f3b4-83d1-4216-a271-f1bbbe2ed67a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d407efb6-37c9-4815-a4b7-4ed9e12a1dbf",
   "metadata": {},
   "source": [
    "### 순차 데이터셋을 나누는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "319cfaeb-6e85-4df6-b81e-d3eeb3a605ee",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97cdd92-68cb-4167-81f3-81b60637428d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
